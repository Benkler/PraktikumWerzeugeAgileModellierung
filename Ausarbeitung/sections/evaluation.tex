\chapter{Evaluation}
\label{ch:Evalutation}

To answer \textit{RQ1.3}, a \textit{Goal Quality Metrics Plan } (GQM) is to be created that specifies the key aspects of the evaluation. 
Chapter \ref{ch:SolutionApplication} applies the identification approach to the running example. As a result, a set of microservices is identified where each microservice provides some functionalities and administers data objects.\\
In order to evaluate this approach, those results are compared to two reference sets. In doing so, not only is it checked whether the right services have been found, but also whether the functionality and the data objects have been divided up correctly. During the evaluation, those aspects are examined independently. Functionality in this sense is a service offered by a microservice that is comprehensible and understood by non-technical stakeholders. \\
The rest of the chapter is structured as follows: First, the evaluation goals and metrics are introduced. Second, the design of the evaluation is demonstrated, including the presentation of two reference sets which are used to compare the output of the approach, when applied to the running example. Third, the evaluation results are presented. Second to last, the evaluation results are discussed. Eventually, the threats to the validity are outlined.


\section{Evaluation Goals and Metrics}

\subsection{Evaluation Goals}
\label{sec:Evaluation:GQM}
Basili \textit{et al.} originally proposed the \textit{GQM Plan} (Goal Quality and Metrics) as a paradigm in software engineering to create specific quality models \cite{BasiliGQM}. \\
The main purpose of the \textit{GQM Plan} is to identify the right metrics to assess the quality of an object in a particular environment. This should prevent the gathering of unnecessary metrics and measurements and consequently reduce the expenditure of work. \\
The \textit{GQM Plan}, as an appropriate method to structure an evaluation, is used to define suitable goals, questions and metrics, which is why it is the means of choice in this evaluation.\\
The \textit{GQM Plan} is a Top-Down approach and is divided in three fundamental steps that precede the measurement and evaluation of results. First, the goal of the evaluation is defined on a conceptual level. Second, questions are delineated to achieve the specific goal. Finally, to answer the questions in a measurable way, metrics have to be defined that are associated with the questions. \\
In the following, the \textit{GQM Plan} for the consecutive evaluation is detailed out:

\begin{itemize}
	\item \textbf{G1:} Determination of the accuracy of the approach to demonstrate that it is capable to identify adequate microservice candidates.
	\item \textbf{G1.Q1:} What is the \textit{Precision and Recall} of the identified microservices compared to the reference sets?
	\item \textbf{G1.Q2:} What is the \textit{Precision and Recall} regarding the functionality of the identified microservices compared to the reference sets?
	\item \textbf{G1.Q3:} What is the \textit{Precision and Recall} regarding data objects each microservice administers compared to the reference sets?


\end{itemize}




\subsection{Evaluation Metrics}
\label{sec:Evaluation:Metrics}

Using metrics is mandatory to measure the quality of the elaborated approach. In this case, it is required to choose a metric that is capable of classifying a set of instances regarding their relevance.
In regard to the following evaluation, those instances are either microservices, functionalities offered by microservices or data objects administered by microservices. Two reference sets are available as further depicted in Sec.\ref{sec:Evaluation:ReferenceSets}. \\
A metric that is capable to measure the relevance of a set of instances compared to a reference set is \textit{Precision and Recall}. Subsequently, the proposed metric is briefly presented.

\noindent
\textit{Precision and Recall} is a classification metric that measures the relevance of retrievable items with respect to a reference set \cite{PrecisionRecall}. Commonly, two distinctions for items in the reference set are made: First, Retrieved or not Retrieved. More precisely, an item is retrieved if it is part of the selected items and vice versa. Secondly, Relevant or Not Relevant. As a result, all retrievable items belong to exactly one of four cells in the following matrix:


\begin{table}[!h]
	\centering
	\begin{tabular}{|l||l|l|l|}
		\hline
		& Relevant & Not Relevant & Sum \\ \hline
		Retrieved     &     $N_{ret\cap rel}$     &     $N_{ret\cap \overline{rel}}$            &     $N_{ret}$  \\ \hline
		Not Retrieved &      $N_{\overline{ret}\cap rel}$      &      $N_{\overline{ret}\cap \overline{rel}}$          &    $N_{\overline{ret}}$   \\\hline
		Sum           &         $N_{rel}$   &      $N_{\overline{rel}}$          &    $N_{total}$   \\ \hline
		
	\end{tabular}
\caption{Retrieval Matrix, Source: \cite{PrecisionRecall}}
    \label{tab:PrecRecall}
    
\end{table}



\noindent
\textbf{Recall} describes the completeness of the retrieval. In other words, how many relevant items are selected in regard to all possible relevant items.

\begin{centering}
	\vspace{1cm}
	
	$Recall=\dfrac{N_{ret\cap rel}}{ N_{rel} }  $
	
	\vspace{1cm}
\end{centering} 

\noindent
\textbf{Precision} illustrates the purity of the retrieval because it puts into proportion the number of retrieved relevant items and the number of all retrieved items.

\begin{centering}
	\vspace{1cm}
	
	$Precision=\dfrac{N_{ret\cap rel}}{ N_{ret} }  $
	
	\vspace{1cm}
\end{centering} 

 
\noindent
In both cases, the values range from zero to one, where a higher value represents a more satisfying value in terms of completeness and purity of the retrieval.
It is important to notice that $N_{\overline{ret}}$ and $N_{\overline{rel}}$ are not part of the formulas. With that in mind, it is possible to apply \textit{Precision and Recall} to the prevalent evaluation scenario. With respect to Table \ref{tab:PrecRecall}, the reference set used forms the relevant items, or $N_{rel}$. Accordingly, $N_{ret}$ constitutes the retrieved items which are either the actual microservices, the functionality provided by the microservices or the data objects administered by the microservices.
The remaining parts which are the non-relevant and non-retrieved items ($N_{\overline{ret}\cap \overline{rel}}$) are unimportant. The following list draws the analogy between Table \ref{tab:PrecRecall} and the predominant evaluation scenario:
\begin{itemize}
	\item \textbf{True Positives:}  $N_{ret\cap rel}$  
	
	\begin{itemize}
		\item Identified microservices that have a similar partner in the reference set \textbf{OR}
		\item Identified functionality that is assigned to the appropriate microservice \textbf{OR}
		\item Identified data objects that are assigned to the appropriate microservice
	\end{itemize}
	
	
	\item  \textbf{False Positives:}  $N_{ret\cap \overline{rel}}$ 
	\begin{itemize}
		\item Identified microservices that do not have a similar partner in the reference set \textbf{OR}
		\item Identified functionality, that is assigned to the wrong microservice  \textbf{OR}
		\item Identified data objects, that are assigned to the wrong microservice
	\end{itemize}
	
	\item \textbf{False Negatives}:  $N_{\overline{ret}\cap rel}$ 
		\begin{itemize}
		\item Microservices in the reference set that are not discovered by the proposed approach  \textbf{OR}
		\item Functionality in the reference microservices which is not discovered and allocated by the proposed approach  \textbf{OR}
		\item Data objects in the reference microservices which are not discovered and allocated by the proposed approach
	\end{itemize}
	
	\item \textbf{True Negatives}\footnote{Note, that this amount consists of all imaginable microservices and is therefore an infinite set. As it is not used to calculate either of the metrics, it is negligible. }: $N_{\overline{ret}\cap \overline{rel}}$
	 	\begin{itemize}
	 	\item  Microservices that are neither discovered by the approach, nor part of the reference set   \textbf{OR}
	 	\item Functionality that is neither distributed into a microservice, nor part of any microservice of the reference set  \textbf{OR}
	 	\item Data objects that are neither distributed into a microservice, nor part of any microservice of the reference set
	 \end{itemize}
	
	
	
\end{itemize}


%"l, b, r, t"
\begin{figure}[!h]
	\includegraphics[width=\textwidth, trim={6cm 5.5cm 11cm 0.5cm}]{img/PrecisionRecall.pdf}
	\caption{Precision and Recall for Microservices}
	\label{fig:PrecisionRecall}
\end{figure}

\pagebreak

\noindent
Since the \textit{Precision and Recall} metric not only gives information about how many microservices, functionalities and data objects have been identified in total, but also about how many of them are correct, it is well suited to determine the accuracy of the result compared to a reference set. \\
Hence, it is necessary to determine the \textit{Precision and Recall} of the identified microservices, the identified functionalities and the identified data objects to answer \textbf{G1.Q1-G1.Q3}.

\section{Evaluation Design}
Evaluations can be structured and conducted in several ways. It is therefore necessary to specify the type of evaluation. This is followed by the presentation of the two reference sets.

\subsection{Evaluation Setup}
The primary goal of the evaluation is to determine the accuracy of the elaborated approach in order to answer \textbf{RQ1.3}. To achieve this, an \textit{Outcome Evaluation} is an appropriate method \cite{Evaluation}, as it shows how effective the approach is in terms of the expected results which are in this case adequate microservices. Therefore, the approach is applied to \textit{CoCoME} and the outcome is compared to two reference sets. This is a well established procedure to reason about the accuracy of the approach. \\
\textit{CoCoME} was initiated in a GI Dagstuhl research seminar as a community case study for software architecture, modelling and analysis.
In recent years, it has been applied in various areas as a demonstrator for software evolution methods\cite{CoCoMETechnical}. \textit{CoCoME} is structured like a typical, distributed and component-based information system and is therefore well suited as a demonstrator for this new type of software evolution - the migration to a microservice-based software architecture. 




\subsection{Reference Sets}
\label{sec:Evaluation:ReferenceSets}
To evaluate the approach, the identified set of microservices (cf. Sec.\ref{sec:Evalutation:Results}) is compared to two alternative decompositions of CoCoME: First, a decomposition proposed by an approach which identifies microservices by using functional decomposition\cite{FunctionalDecompositionHeinrich} and second, a set of microservices which was manually identified\cite{NikoCoCoMEImpl}. \\
In both cases the microservices are listed together with the functionalities they offer and the data objects they manage. Functionalities and data objects are listed behind the bullet points. \\

\noindent
\textbf{Reference Set 1: Functional Decomposition Approach} \\
In \textit{Identifying Microservices Using Functional Decomposition} \cite{FunctionalDecompositionHeinrich}, a systematic approach to find an appropriate division of a system into microservices is presented. It also uses CoCoME to evaluate the approach it provides. \\
As mentioned in Sec.\ref{sec:stateOfTheArt:comparison}, the compulsory and non-trivial revision of nouns and verbs to eliminate synonyms etc. is a substantial disadvantage.\\
Regarding the evaluation, Tyszberowicz \textit{et al.} claim that their approach identifies good microservices for a microservice-based system decomposition of CoCoME. The aforementioned evaluation includes a comparison to three independent software projects that implemented CoCoME. Two groups identified, apart from the naming, a similar set of microservices. The third group identified a more detailed decomposition of CoCoME, but a revision reveals that the additional microservices are only a refinement of the proposed microservices. \\
However, the evaluation lacks profundity: Microservices are only compared on a more abstract level, since it is not checked whether functionality and data objects are assigned to the correct microservice. The following microservices are identified: 


\begin{multicols}{2}
	\textbf{Sale}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Handle Sale
			\item Handle Payment
			\item Identify stock Item
			\item \textbf{Data objects}: -  
		\end{itemize}
	\end{flushleft}
	
	
	\vfill
	\columnbreak
	\textbf{ProductList}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Create orders
			\item Change price
			\item \textbf{Data Objects}: product
			
			
		\end{itemize}
	\end{flushleft}
	
\end{multicols}




\begin{multicols}{2}
	\textbf{StockOrder}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Create stock report
			\item Handle inventory
			\item \textbf{Data objects}: stockItem, order
		\end{itemize}
	\end{flushleft}
	
	
	\vfill
	\columnbreak
	\textbf{Reporting}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Create delivery report
			\item Handle deliveries
			\item \textbf{Data Objects}: - 
			
		\end{itemize}
	\end{flushleft}
\end{multicols}



\noindent
\textbf{Reference Set 2: Manual Decomposition} \\
In the course of this thesis, a microservice-based version of CoCoME was implemented. The microservice identification process itself was finished before the literature review for the thesis started. 
Moreover, the implementation was not influenced by the microservice decomposition proposed by Tyszberowicz \textit{et al.} as it was not familiar by the time the implementation process started. Therefore, the process was non-biased. \\


The identification process itself was conducted manually and supported by the previous knowledge of the CoCoME domain. Once more, the time consuming and difficult identification process clarified the necessity of a structured approach to identify microservices. Beside the use case specification, a monolithic implementation of CoCoME (the Hybrid Cloud-based Variant \cite{CoCoMETechnical}) was used as additional information resource to discover requirements, functionality and dependencies. Subsequently, the system was decomposed into loosely coupled and highly cohesive microservices. \\
The following four microservices were identified:




\begin{multicols}{2}
	\textbf{Stores- and Sale}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Handle sale
			\item Handle payment
			\item Manage express checkout
			\item Exchange products
			\item Identify stock items
			\item Handle inventory
			\item Change price
			\item Handle stores
			\item Handle enterprises
			\item Handle deliveries
			\item \textbf{Data objects}: stockItem, store, enterprise
		\end{itemize}
	\end{flushleft}
	
	
	\vfill
	\columnbreak
	\textbf{Product}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Show products
			\item Create products
			\item Show suppliers
			\item Create suppliers
			\item \textbf{Data Objects}: product, supplier
		
		
		\end{itemize}
	\end{flushleft}

\end{multicols}




\begin{multicols}{2}
	\textbf{Order}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Show orders
			\item Create orders
			\item \textbf{Data objects}: order
		\end{itemize}
	\end{flushleft}
	
	
    	\vfill
	\columnbreak
	\textbf{Reports}
	\begin{flushleft}
		\begin{itemize}[noitemsep]
			\item Create delivery report
			\item Create stock report
			\item \textbf{Data Objects}: report  
	
		\end{itemize}
	\end{flushleft}
\end{multicols}





\noindent
\textbf{Differences between both sets} \\
The reference sets contain the same amount of microservices. Apart from the naming, those microservices are similar: one service is responsible for the sales process, another manages the products, a third one is responsible for the orders and the last one is in charge of the reporting. \\
However, both differ with regard to the functionality offered and the managed data objects. Obviously, \textit{Reference Set 1} features nine functionalities and three data objects whereas \textit{Reference Set 2} contains 18 functionalities and seven data objects. It is important to note that all functionalities and data objects of the first reference set are part of the second one. Since the second reference set corresponds to a real implementation, it can be assumed that the first set is not complete.\\
Furthermore, the placement of functions and data objects contained in both reference sets is also different. For example, in \textit{Reference Set 1} the data object \textit{stockItem} is part of the order microservice, whereas in \textit{Reference Set 2} it is managed by the sale's microservice.\\
Another example is the assignment of functionalities \textit{Create Stock Report}  and \textit{Create Delivery Report}: In \textit{Reference Set 1} \textit{Create Stock Report}  is part of the report microservice and \textit{Create Delivery Report} part of the order microservice whereas in \textit{Reference Set 2} both are assigned to the report service. From a domain point of view, the latter is the more meaningful assignment. This example indicates a discrepancy between \textit{Reference Set 1} and \textit{Reference Set 2} where the latter corresponds to a real implementation. \\
Thus, a stronger focus is placed on the comparison with the second reference set.




\section{Evaluation Results}
\label{sec:Evalutation:PrecisionAndRecallMeasurement}
In this section, the precision and recall metric (cf. Sec.\ref{sec:Evaluation:Metrics}) for the results of the approach (cf. Sec.\ref{sec:SolutionApplication:ExtractMicroserviceCandidates}) is calculated using the reference sets described in Sec.\ref{sec:Evaluation:ReferenceSets}. 
For each reference set, the precision and recall instances are i) identified microservices, ii) identified functionality offered by a microservice and iii) identified data objects administered by a microservice. 

\subsection{Reference Set 1}

\textbf{Microservices:} The approach identified three services, \textit{Microservice 1} is similar to \textit{Sale} and \textit{Microservice 2} is comparable to \textit{Reporting}. \textit{Microservice 3}, on the other hand, can neither be clearly assigned to \textit{StockOrder} nor to \textit{ProductList}, since it unites the functionalities of both. More specifically, \textit{Microservice 3} is a more coarse-grained microservice that consists of \textit{StockOrder} and \textit{ProductList}. Therefore it can be concluded that three out of four microservices were successfully identified and hence, \textit{true positive}. None of the identified microservices are \textit{false positive}(cf. Sec.\ref{sec:Evaluation:Metrics}).

\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
		\vspace{0.5cm}
	\flushleft

		
	$Recall_{microservice}=\dfrac{3}{4} = 0.75  $
		\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft

		
	$Precision_{microservice}=\dfrac{3}{3} = 1  $
		\vspace{0.5cm}
	
\end{minipage}

\noindent
\textbf{Functionality:} Since \textit{Microservice 3} consists of the microservices \textit{StockOrder} and \textit{ProductList}, functions assigned to this microservice are recognized as \textit{true positive } if they appear in one of the two microservices \textit{StockOrder} and \textit{ProductList}.
With this in mind, the results are as follows: The reference set counts nine functionalities. The approach identified 13 functionalities, from which five are assigned to the right service. Hence, the precision and recall values are:

\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Recall_{functionality}=\dfrac{5}{9} \approx 0.56  $
	\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Precision_{functionality}=\dfrac{5}{13} \approx 0.38  $
	\vspace{0.5cm}
	
\end{minipage}



\noindent
\textbf{Data Objects:} Regarding \textit{Microservice 3}, the same line of reasoning is applied to the identified data objects: The reference set counts three data objects. The approach identified seven, from which 2 are assigned to the right service. In this case, the precision and recall values are:


\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Recall_{dataObject}=\dfrac{2}{3} \approx 0.67  $
	\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Precision_{dataObject}=\dfrac{2}{7} \approx 0.29  $
	\vspace{0.5cm}
	
\end{minipage}





\subsection{Reference Set 2}



\textbf{Microservices:} As described in Sec.\ref{sec:Evaluation:ReferenceSets}, both reference sets contain similar microservices, which differ only by the names at microservice level. The functionalities and data objects, however, are different. Hence, the precision and recall value are identical in regard to identified microservices:

\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Recall_{microservice}=\dfrac{3}{4} = 0.75  $
	\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Precision_{microservice}=\dfrac{3}{3} = 1  $
	\vspace{0.5cm}
	
\end{minipage}

\noindent
\textbf{Functionality:} Again \textit{Microservice 3} is considered to be a coarse grained union of the microservice \textit{Product} and \textit{Order}. In this case, the reference set counts 18 functionalities. The approach identified 13 functionalities, from which 12 are assigned to the right service. Hence, the precision and recall values are:

\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Recall_{functionality}=\dfrac{12}{18} \approx 0.67  $
	\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Precision_{functionality}=\dfrac{12}{13} \approx 0.92  $
	\vspace{0.5cm}
	
\end{minipage}


\noindent
\textbf{Data Objects:} The reference set counts seven data objects. The approach identified seven, from which 5 are assigned to the right service. In this case, the precision and recall values are:


\hspace{1cm}
\noindent
\begin{minipage}{.4\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Recall_{dataObject}=\dfrac{5}{7} \approx 0.71  $
	\vspace{0.5cm}
	
\end{minipage}%
\begin{minipage}{.5\linewidth}
	\vspace{0.5cm}
	\flushleft
	
	
	$Precision_{dataObject}=\dfrac{5}{7} \approx 0.71  $
	\vspace{0.5cm}
	
\end{minipage}

\section{Discussion of the Evaluation Results}
\label{sec:Evalutation:Results}
The measurement of the precision and recall metric in the previous section gives information about the accuracy of the approach. Each of the identified microservices is also represented in the reference sets, which leads to a maximum precision of \textit{1.0}. Due to the coarse-grained \textit{Microservice 3} the recall is at 0.75. \\
However, it can be assumed that the clustering algorithm used is not capable of identifying comparatively small clusters, as it merges them to a more coarse-grained microservice. It must be further investigated why the approach combines the comparatively small microservices \textit{Order} and \textit{Product} into the coarse-grained \textit{Microservice 3}. This is further discussed in Sec.\ref{sec:Conclusion:LimitsFutureWork}. Nevertheless, it has to be noticed that the approach does not produce \textit{false positive} microservices. \\
Regarding the functionality and data objects, the comparisons to the reference sets produces different values for recall and precision:\\

\noindent
Compared to the first reference set, the approach produces less 
satisfying values. Only about half (Recall = 0.56) of the functionality in the reference set is correctly identified.  In addition, only five out of 13 identified functionalities are assigned to the right service which leads to a small precision of 0.38. Regarding the data objects, the approach identifies two out of three correctly with an overall identification of seven data objects. Hence, the recall is at a satisfiable level of 0.67 but the precision is very low at 0.29. However, these non-satisfying values can be explained by the constitution of \textit{Reference Set 1} as described in Sec.\ref{sec:Evaluation:ReferenceSets}. The discrepancy between this reference set and a comparable implementation (\textit{Reference Set 2}) can explain the low recall value of the functionality. The incompleteness in regard to the data objects and the functionality explains the low precision values, as the approach identifies functionality and data objects which are not part of the reference set but obviously part of the CoCoME domain.\\

\noindent
Compared to the second reference set, the values for precision and recall are much better. Almost every identified functionality is assigned to the right microservice, which leads to a precision value of 0.92. Obviously the \textit{Reference Set 2} contains additional functionality that is not identified by the approach. As a consequence the recall value is not as high as the precision. However, it should be noted that the second reference set was created by using additional sources of information such as the source code whereas the approach only uses the provided use cases. With respect to the data objects, both precision and recall are at a satisfying value of 0.71. Finally, it has to be noticed that the approach does not identify any functionalities or data objects that are not part of \textit{Reference Set 2}. Only a few instances are assigned to wrong services. \\

\noindent
The assignment of the functionality and the data objects is decisive for the quality of the microservices, because an incorrect assignment would result in a high number of intra-service calls. As mentioned in the previous section, \textit{Reference Set 1} was only evaluated on an abstract level, whereby it was not checked whether functionalities and data objects were assigned correctly. However, the quality of the evaluation depends on the quality of the reference sets. It is therefore questionable to what extent \textit{Reference Set 1} should be used to evaluate the approach.\\
Since a stronger focus is placed on the comparison with the second reference set, which produced satisfactory results, the accuracy of the approach is considered to be satisfying.

 






\section{Threats to Validity}
\label{sec:Evalutation:ThreatsToValidity}
To assess the overall validity of the evaluation results it is necessary to asses threats to the internal and external validity. 


\subsection{Internal Validity}
Internal validity describes to what extend the evaluation allows unfounded results, usually as a result of systematic errors and bias \cite{Validity}. \\
First of all, \textit{Reference Set 2} is based on a manual implementation. It is not claimed that this implementation represents the best or the only way to decompose CoCoME into appropriate microservices. However, the identification process was based on a sufficient knowledge about the CoCoME domain and the microservice topic. As described in Sec.\ref{sec:Evaluation:ReferenceSets}, the reference sets are different with regard to the functionality offered and the managed data objects. Especially the suitability of the first reference set is questionable, as depicted in the previous section. Evaluation quality, however, depends on the quality of the reference sets. In order to confirm the result, it is therefore necessary to consult other reference sets or have the existing ones examined by unbiased domain experts.\\
Further, in order to compare the results with the reference sets, it was necessary to identify similar functionalities and data objects (cf. Sec.\ref{sec:SolutionApplication:ExtractMicroserviceCandidates}) that appear in each of them. A biased fitting between the available instances in the reference sets and the result of the approach cannot be ruled out.\\
Another threat to the validity is the claim that \textit{Microservice 3} consists of the microservices \textit{Order} and \textit{Product}. Functions and data object that appear in \textit{Microservice 3} are regarded to be successfully identified if they are in either of the two reference microservices. Strictly speaking, only two of the four reference services are identified by the approach. Consequently, all functionalities and data objects assigned to \textit{Microservice 3} would be false positives. This, however, distorts the result in the same negative way, since most of the functions and data objects were assigned correctly, but to the coarse-grained \textit{Microservice 3}.




\subsection{External Validity}
In contrast, external validity describes whether the evaluation results can be generalized and applied to other contexts \cite{Validity}. That is to say, the approach also produces good microservice candidates for other component-based information systems like \textit{CoCoME}. \\
During the development of the approach, no domain restrictions were wittingly enforced. All assumptions were made with the intention of being context-independent. Further, \textit{CoCoME} is structured like a typical, distributed and component-based information system. It is therefore to be expected that the approach also produces adequate results for other information systems. \\
Despite all the precautions, it is nevertheless possible that a fitting between \textit{CoCoME} and the developed approach has taken place, so that the approach only produced adequate microservices in this case. It is therefore necessary to apply the approach to other systems to gather further insights on the accuracy.










